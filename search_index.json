[
["index.html", "e-Rum2020 Program Index", " e-Rum2020 Program e-Rum2020 Organizing Committee 2020-09-11 20:19:36 Index Book of abstracts for accepted contributions Detailed program schedule booklet Conference materials (collected in a GitHub repository) "],
["erum2020-contributed-program.html", "1 eRum2020 Contributed Program", " 1 eRum2020 Contributed Program Overview of accepted contributions organized by session type. "],
["lightning-talks.html", "1.1 Lightning talks", " 1.1 Lightning talks 1.1.1 An enriched disease risk assessment model based on historical blood donors records Andrea Cappozzo, PhD student at University of Milan-Bicocca Track(s): R Applications Abstract: Historically, the medical literature has largely focused on determining risk factors at an illness-specific level. Nevertheless, recent studies suggested that identical risk factors may cause the appearance of different diseases in different patients (Meijers &amp; De Boer, 2019). Thanks to the joint collaboration of Heartindata, a group of data scientists offering their passion and skills for social good, and Avis Milano, the Italian blood donor organization, an enriched disease risk assessment model is developed. Multiple risk factors and donations drop-out causes are collectively analyzed from AVIS longitudinal records, with the final aim of providing a broader and clearer overview of the interplay between risk factors and associated diseases in the blood donors population. Coauthor(s): Edoardo Michielon, Alessandro De Bettin, Chiara D’Ignazio, Luigi Noto, Davide Drago, Alberto Prospero, Francesca De Chiara, Sergio Casartelli . 1.1.2 rdwd: R interface to German Weather Service data Berry Boessenkool, R trainer &amp; consultant Track(s): R Applications Abstract: rdwd is an R package to handle data from the German Weather Service (DWD). It allows to easily select, download and read observational data from over 6k weather stations. Both current data and historical records (partially dating back to the 1860s) are handled. Since about a year, gridded data from radar measurements can be read as well. 1.1.3 tv: Show Data Frames in the Browser Christoph Sax, R-enthusiast, economist @cynkra Track(s): R World Abstract: The tv package lively displays data frames during data analysis. It modifies the print method of data frames, tibbles or data tables to also appear in a browser or in the view pane of RStudio. This is similar in spirit to the View() function in RStudio, works in other development environments, and has several advantages. Changes in data frame are shown immediately and next to the script and the console output, rather than on top of them. The display keeps the position and the width of columns if a modified data frame is shown in tv. It is updated asynchronously, without interrupting the analysis workflow. Coauthor(s): Kirill Müller . 1.1.4 Predicting the Euro 2020 results using tournament rank probabilities scores from the socceR package Claus Ekstrøm, Statistician at University of Copenhagen. Longtime R hacker. Track(s): R Applications Abstract: The 2020 UEFA European Football Championship will be played this summer. Football championships are the source of almost endless predictions about the winner and the results of the individual matches, and we will show how the recently developed tournament rank probability score can be used to compare predictions. Different statistical models form the basis for predicting the result of individual matches. We present an R framework for comparing different prediction models and for comparing predictions about the Euro results. Everyone is encouraged to contribute their own function to make predictions for the result of the Euro 2020 championship. Each contributer will be shown how to provide two functions: a function that predicts the final score for a match between two teams with different skill levels, and a function that updates the skill levels based on the results of a given match. By supplying these two functions to the R framework the prediction results can be compared and the winner of the best football predictor can be found when Euro 2020 finishes. 1.1.5 Differential Enriched Scan 2 (DEScan2): an R pipeline for epigenomic analysis. Dario Righelli, Department of Statistics, University of Padua, Post-Doc Track(s): R Life Sciences Abstract: We present DEScan2, a R/Bioconductor package for the differential enrichment analysis of epigenomic sequencing data. Our method consists of three steps: peak caller, peak consensus across samples, and peak signal quantification. The peak caller is a standard moving scan window comparing the counts between a sliding window and a larger region outside the window, using a Poisson likelihood, providing a z-score for each peak. However, the package can work with any external peak caller: to this end, we provide additional functionalities to load peaks from bed files and handle them as internal optimized structures. The consensus step aims to determine if a peak is a “true peak” based on its replicability across samples: we developed a filtering step to filter out those peaks not present in at least a user given number of samples. A further threshold can be used over the peak z-scores. Finally, the third step produces a count matrix where each column is a sample and each row a previously filtered peak. The value of each matrix cell is the number of reads for the peak in the sample. Furthermore, our package provides several functionalities for common genomic data structure handling, for instance, to give the possibility to split the data over the chromosomes to speed-up the computations parallelizing them on multiple CPUs. Coauthor(s): Koberstein John, Gomes Bruce, Zhang Nancy, Angelini Claudia, Peixoto Lucia, Risso Davide . 1.1.6 Ultra fast penalized regressions with R package {bigstatsr} Florian Privé, Postdoc at Aarhus University Track(s): R Machine Learning &amp; Models Abstract: In this talk, I introduce the implementations of penalized linear and logistic regressions as implemented in R package {bigstatsr}. These implementations use data stored on disk to handle very large matrices. They automatically perform a procedure similar to cross-validation to choose the two hyper-parameters, λ and α, of the elastic net regularization, in parallel. They employ an early stopping criterion to avoid fitting very expensive models, making these implementations on average 10 times faster than with {glmnet}. However, package {bigstatsr} does not implement all the many models and options provided by the excellent package {glmnet}; some are area of future development. 1.1.7 Supporting Twitter analytics application with graph-databases and the aRangodb package Gabriele Galatolo, Kode Srl, Software Developer &amp; Data Scientist Track(s): R Applications Abstract: The importance of finding efficient ways to model and to store unstructured data has incredibly grown in the last decade, in particular with the strong expansion of social-media services. Among those storing tools an increasingly important class of databases is represented by the graph-oriented databases, where relationships between data are considered first-class citizens. In order to support the analyst or the data scientist to interact and use in a simple way with this paradigm, we developed last year the package aRangodb, an interface with the graph-oriented database ArangoDB. To show the capabilities of the package and of the underlying way to model data using graphs we present Tweetmood, a tool to analyze and visualize tweets from Twitter. In this talk, we will present some of the most significant features of the package applied in the Tweetmood context, such as functionalities to traverse the graph and some examples in which the user can elaborate those graphs to get new information that can easily be stored using the functions and the tools available in the package. Coauthor(s): Francesca Giorgolo, Ilaria Ceppa, Marco Calderisi, Davide Massidda, Matteo Papi, Andrea Spinelli, Andrea Zedda, Jacopo Baldacci, Caterina Giacomelli . 1.1.8 Reproducible Data Visualization with CanvasXpress Ger Inberg, Freelance Analytics Developer Track(s): R Dataviz &amp; Shiny Abstract: canvasXpress was developed as the core visualization component for bioinformatics and systems biology analysis at Bristol-Myers Squibb. It supports a large number of visualizations to display scientific and non-scientific data. canvasXpress also includes a simple and unobtrusive user interface to explore complex data sets, a sophisticated and unique mechanism to keep track of all user customization for Reproducible Research purposes, as well as an ‘out of the box’ broadcasting capability to synchronize selected data points in all canvasXpress plots in a page. Data can be easily sorted, grouped, transposed, transformed or clustered dynamically. The fully customizable mouse events as well as the zooming, panning and drag-and-drop capabilities are features that make this library unique in its class. 1.1.9 Design your own quantum simulator with R Indranil Ghosh, Final year post Graduate student from the department of Physics, Jadavpur University, Kolkata, India Track(s): R Applications Abstract: The main idea of the project is to use the R ecosystem to write computer codes for designing a quantum simulator, for simulating different quantum algorithms. I will start with giving a brief introduction to linear algebra for starting with quantum computation, and how to write your own R codes from scratch to implement them. Then I will take a dive into implementing simple quantum circuits starting with initializing qubits and terminating with a measurement. I will also implement simple quantum algorithms concluding with giving a brief intro to quantum game theory and their simulations with R. 1.1.10 What are the potato eaters eating Keshav Bhatt, R-fan and independent researcher Track(s): R Applications, R Dataviz &amp; Shiny Abstract: Although stereotypes can quite useful they are often not correct. For instance, the Dutch are stereotyped as being potato eaters. While this might have been historically correct, it is not currently accurate. The Dutch sparingly eat potatoes and this paper uses data to disprove the stereotype. To get an impression of Dutch food habits, a popular local website was scraped. Besides its popularity, the website hosts user-generated content, giving a good proxy of Dutch taste-buds. While it was apparent on the website, lasagna is the most popular dish. Detailed NLP analysis of more than 50,000 recipes showed that potato based dishes are in fact nowhere at the top. This vindicated my belief. Moreover, it shows that the Dutch kitchen is globalizing. Tomato, a hallmark of South Europe is more popular than the Dutch potato. Also observed is the popularity of many herbs in the recipes, which are not a traditional component of the Dutch kitchen. The world is changing and our kitchens too. This trend will also be explored for other countries also. 1.1.11 dm: working with relational data models in R Kirill Müller, Clean code, tidy data. Consulting for cynkra, coding in the open. Track(s): R Applications, R Production, R World Abstract: Storing all data related to a problem in a single table or data frame (“the dataset”) can result in many repetitive values. Separation into multiple tables helps data quality but requires “merge” or “join” operations. {dm} is a new package that fills a gap in the R ecosystem: it makes working with multiple tables just as easy as working with a single table. A “data model” consists of tables (both the definition and the data), and primary and foreign keys. The {dm} package combines these concepts with data manipulation powered by the tidyverse: entire data models are handled in a single entity, a “dm” object. Three principal use cases for {dm} can be identified: When you consume a data model, {dm} helps access and manipulate a dataset consisting of multiple tables (database or local data frames) through a consistent interface. When you use a third-party dataset, {dm} helps normalizing the data to remove redundancies as part of the cleaning process. To create a relational data model, you can prepare the data using R and familiar tools and seamlessly export to a database. The presentation revolves around these use cases and shows a few applications. The {dm} package is available on GitHub and will be submitted to CRAN in early February. 1.1.12 Explaining black-box models with xspliner to make deliberate business decisions Krystian Igras, Data Scientists and Software Engineer at Appsilon Track(s): R Machine Learning &amp; Models Abstract: A vast majority of the state of the art ML algorithms are black boxes, meaning it is difficult to understand their inner workings. The more that algorithms are used as decision support systems in everyday life, the greater the necessity of understanding the underlying decision rules. This is important for many reasons, including regulatory issues as well as making sure that the model learned sensible features. You can achieve all that with the xspliner R package that I have created. One of the most promising methods to explain models is building surrogate models. This can be achieved by inferring Partial Dependence Plot (PDP) curves from the black box model and building Generalised Linear Models based on these curves. The advantage of this approach is that it is model agnostic, which means you can use it regardless of what methods you used to create your model. From this presentation, you will learn what PDP curves and GLMs are and how you can calculate them based on black box models. We will take a look at an interesting business use case in which we’ll find out whether the original black box model or the surrogate one is a better decision system for our needs. Finally, we will see an example of how you can explain your models using this approach with the xspliner package for R (already available on CRAN!). 1.1.13 Using open-access data to derive genome composition of emerging viruses Liam Brierley, MRC Skills Development Fellow, University of Liverpool Track(s): R Life Sciences Abstract: Outbreaks of new viruses continue to threaten global health, including pandemic influenza, Ebola virus, and the novel coronavirus ‘nCoV-2019’. Advances in genome sequencing allow access to virus RNA sequences on an unprecedented scale, representing a powerful tool for epidemiologists to understand new viral outbreaks. We use NCBI’s GenBank, a curated open-access repository containing &gt;200 million genetic sequences (3 million viral sequences) directly submitted by users, representing many individual studies. However, the resulting breadth of data and inconsistencies in metadata present consistent challenges. We demonstrate our approach using R to address these challenges and a need for reproducibility as data increases. Firstly, we use rentrez to programmatically search, filter, and obtain virus sequences from GenBank. Secondly, we use taxize to resolve pervasive problems of naming conflicts, as virus names are often recorded differently between entries, partly because virus classification is complex and regularly revised. We successfully resolve 428 mammal and bird RNA viruses to species level before extracting sequences. Obtaining genome sequences of a large inventory of viruses allows us to estimate genomic composition biases, which show promise in predicting virus epidemiology. Ultimately, this pathway will allow better quantification of future epidemic threats. Coauthor(s): Anna Auer-Fowler, Maya Wardeh, Matthew Baylis, Prudence Wong . 1.1.14 A principal component analysis based method to detect biomarker captation from vibrational spectra Marco Calderisi, Kode srl, CTO Track(s): R Life Sciences Abstract: BRAIKER is a microfluidics-Based biosensor aimed to detect biomarkers. The device is responsive to changes of mass and viscosity over its surface. When selected markers react with the sensor, a variation of resonant acoustic frequencies (called harmonics) is produced. A serious problem when examining the data produced by biosensors is the subjectivity of standard method to evaluate the pattern of harmonics. In our research, a method based on the principal component analysis has been applied on vibrational data. An R-Shiny application was developed in order to present data visualizations and multivariate analyses of vibrational spectra. The Shiny application allows to clean and explore data by using interactive data visualisation tools. The principal component analysis is applied to analyse simultaneously the full set of frequencies for multiple experimental runs, reducing the multivariate data set into a small number of components accounting for a component of variance near to that the original data. Functionalised and non-functionalised resonating foils of biosensor can be classified in order to validate the capability of the device to detect biomarkers, lowering the LOD and increasing sensitivity and resolution. Coauthor(s): Francesca Giorgolo, Ilaria Ceppa, Davide Massidda, Matteo Papi, Gabriele Galatolo, Andre Spinelli, Andrea Zedda, Jacopo Baldacci, Caterina Giacomelli, marco cecchini, matteo agostini . 1.1.15 An innovative way to support your sales force Matilde Grecchi, Head of Data Science &amp; Innovation @ZucchettiSpa Track(s): R Production, R Dataviz &amp; Shiny, R Machine Learning &amp; Models, R Applications Abstract: Explanation of the web application realized in Shiny and deployed in production to support the sales force of Zucchetti. An overview of the overall step followed from data ingestion to modeling, from validation of the model to shiny web-app realization, from deployment in production to continous learning thanks to feedbacks coming from sales force and redemption of customers. All the code is written in R using RStudio. The deployment of the app is done with ShinyProxy.io 1.1.16 ptmixed: an R package for flexible modelling of longitudinal overdispersed count data Mirko Signorelli, Dept. of Biomedical Data Sciences, Leiden University Medical Center Track(s): R Machine Learning &amp; Models, R Life Sciences Abstract: Overdispersion is a commonly encountered feature of count data, and it is usually modelled using the negative binomial (NB) distribution. However, not all overdispersed distributions are created equal: while some are severely zero-inflated, other exhibit heavy tails. Mounting evidence from many research fields suggests that often NB models cannot fit sufficiently well heavy-tailed or zero-inflated counts. It has been proposed to solve this problem by using the more flexible Poisson-Tweedie (PT) family of distributions, of which the NB is special case. However, current methods based on the PT can only handle cross-sectional datasets and no extension for correlated data is available. To overcome this limitation we propose a PT mixed-effects model that can be used to flexibly model longitudinal overdispersed counts. To estimate this model we develop a computational pipeline that uses adaptive quadratures to accurately approximate the likelihood of the model, and numeric optimization methods to maximize it. We have implemented this approach in the R package ptmixed, which is published on CRAN. Besides showcasing the package’s functionalities, we will present an assessment of the accuracy of our estimation procedure, and provide an example application where we analyse longitudinal RNA-seq data, which often exhibit high levels of zero-inflation and heavy tails. Reference: Signorelli, M., Spitali, P., Tsonaka, R. (2020, in press). Poisson-Tweedie mixed-effects model: a flexible approach for the analysis of longitudinal RNA-seq data. To appear in Statistical Modelling. arXiv preprint: arXiv:2004.11193 Coauthor(s): Roula Tsonaka, Pietro Spitali . 1.1.17 One-way non-normal ANOVA in reliability analysis using with doex Mustafa CAVUS, PhD Student @Eskisehir Technical University Track(s): R Production, R Life Sciences, R Applications Abstract: One-way ANOVA is used for testing equality of several population means in statistics, and current packages in R provides functions to apply it. However, the violation of its assumptions are normality and variance heterogeneity limits its use, also not possible in some cases. doex provides alternative statistical methods to solve this problem. It has several tests based on generalized p-value, parametric bootstrap and fiducial approaches for the violation of variance heterogeneity and normality. Moreover, it provides the newly proposed methods for testing equality of mean lifetimes under different failure rates. This talk introduces doex package provides has several methods for testing equality of population means independently the strict assumptions of ANOVA. An illustrative example is given for testing equality of mean of product lifetimes under different failure rates. Coauthor(s): Berna YAZICI . 1.1.18 Keeping on top of R in Real-Time, High-Stakes trading systems Nicholas Jhirad, Senior Data Scientist, CINQ ICT (on Contract to Pinnacle Sports) Track(s): R Production Abstract: Visibility is the key to production. For R to work inside that environment, we need ubiquitous logging. I’ll share insights from our experience building a production-grade R stack and monitoring all of our R applications via syslog, the ‘rsyslog’ package (on CRAN) and splunk. Coauthor(s): Aaron Jacobs . 1.1.19 Towards more structured data quality assessment in the process mining field: the DaQAPO package Niels Martin, Postdoctoral researcher Research Foundation Flanders (FWO) - Hasselt University Track(s): R Applications Abstract: Process mining is a research field focusing on the extraction of insights on business processes from process execution data embedded in files called event logs. Event logs are a specific data structure originating from information systems supporting a business process such as an Enterprise Resource Planning System or a Hospital Information System. As a research field, process mining predominantly focused on the development of algorithms to retrieve process insights from an event log. However, consistent with the “garbage in - garbage out”-principle, the reliability of the algorithm’s outcomes strongly depends upon the data quality of the event log. It has been widely recognized that real-life event logs typically suffer from a multitude of data quality issues, stressing the need for thorough data quality assessment. Currently, event log quality is often judged on an ad-hoc basis, entailing the risk that important issues are overlooked. Hence, the need for a more structured data quality assessment approach within the process mining field. Therefore, the DaQAPO package has been developed, which is an acronym for Data Quality Assessment of Process-Oriented data. It offers an extensive set of functions to automatically identify common data quality problems in process execution data. In this way, it is the first R-package which supports systematic data quality assessment for event data. Coauthor(s): Niels Martin (Research Foundation Flanders FWO - Hasselt University), Greg Van Houdt (Hasselt University), Gert Janssenswillen (Hasselt University) . 1.1.20 Analyzing Preference Data with the BayesMallows Package Øystein Sørensen, Associate Professor, University of Oslo Track(s): R Machine Learning &amp; Models Abstract: BayesMallows is an R package for analyzing preference data in the form of rankings with the Mallows rank model, and its finite mixture extension, in a Bayesian framework. The model is grounded on the idea that the probability density of an observed ranking decreases exponentially with the distance to the location parameter. It is the first Bayesian implementation that allows wide choices of distances, and it works well with a large number of items to be ranked. BayesMallows handles non-standard data: partial rankings and pairwise comparisons, even in cases including non-transitive preference patterns. The Bayesian paradigm allows coherent quantification of posterior uncertainties of estimates of any quantity of interest. These posteriors are fully available to the user, and the package comes with convenient tools for summarizing and visualizing the posterior distributions. This talk will focus on how the BayesMallows package can be used to analyze preference data, in particular how the Bayesian paradigm allows endless possibilities in answering questions of interest with the help of visualization of posterior distributions. Such posterior summaries can easily be communicated with scientific collaborators and business stakeholders who may not be machine learning experts themselves. Coauthor(s): Marta Crispino, Qinghua Liu, Valeria Vitelli . 1.1.21 Predicting Business Cycle Fluctuations Using Text Analytics Sami Diaf, Researcher at the University of Hamburg Track(s): R Machine Learning &amp; Models Abstract: The use of computational linguistics proved to be crucial in studying macroeconomic forecasts and understanding the essence of such exercises. Combining machine learning algorithms with text mining pipelines helps dissecting potential patterns of forecast errors and investigates the role of ideology in such outcomes. The Priority Program “Exploring the Experience-Expectation Nexus” builds up, from a large database of German business cycle reports, advanced topic models and predictive analytics to investigate the role of ideology in the production of macroeconomic forecasts. The pipelines call for advanced data processing, predicting business fluctuations from text covariates, measuring ideological stances of forecasters and explaining what influences forecast errors. 1.1.22 Flexible deep learning via the JuliaConnectoR Stefan Lenz, Statistician at the Institute of Medical Biometry and Statistics (IMBI), Faculty of Medicine and Medical Center – University of Freiburg Track(s): R Machine Learning &amp; Models Abstract: For deep learning in R, frameworks from other languages, e. g. from Python, are widely used. Julia is another language which offers computational speed and a growing ecosystem for machine learning, e. g. with the package “Flux”. Integrating functionality of Julia in R is especially promising due to the many commonalities of Julia and R. We take advantage of these in the design of our “JuliaConnectoR” R package, which aims at a tight integration of Julia in R. We would like to present our package, together with some deep learning examples. The JuliaConnectoR can import Julia functions, also from whole packages, and make them directly callable in R. Values and data structures are translated between the two languages. This includes the management of objects holding external resources such as memory pointers. The possibility to pass R functions as arguments to Julia functions makes the JuliaConnectoR a truly functional interface. Such callback functions can, e. g., be used to interactively display the learning process of a neural network in R while it is trained in Julia. Among others, this feature sets the JuliaConnectoR apart from the other R packages for integrating Julia in R, “XRJulia” and “JuliaCall”. This becomes possible with an optimized communication protocol, which also allows a highly efficient data transfer, leveraging the similarities in the binary representation of values in Julia and R. Coauthor(s): Harald Binder . 1.1.23 Time Series Missing Data Visualizations Steffen Moritz, Institute for Data Science, Engineering, and Analytics, TH Köln Track(s): R Dataviz &amp; Shiny, R Applications Abstract: Missing data is a quite common problem for time series, which usually also complicates later analysis steps. In order to deal with this problem, visualizing the missing data is a very good start. Visualizing the patterns in the missing data can provide more information about the reasons for the missing data and give hints on how to best proceed with the analysis. This talk gives a short intro into the new plotting functions being introduced with the 3.1 version of the imputeTS CRAN package. Coauthor(s): Thomas Bartz-Beielstein . 1.1.24 effectclass: an R package to interpret effects and visualise uncertainty Thierry Onkelinx, Statistician at the Research Institute for Nature and Forest Track(s): R Dataviz &amp; Shiny Abstract: The package classifies effects by comparing their confidence interval with a reference, a lower and an upper threshold, all of which are set by the user a priori. The null hypothesis is a good choice as reference. The lower and upper threshold define a region around the reference in which the effect is small enough to be irrelevant. These thresholds are ideally based on the effect size used in the statistical power analysis of the design. Otherwise they can be based on expert judgement. The result is a ten-scale classification of the effect. Three classes exist for significant effects above the reference and three classes for significant effects below the reference. The remaining four classes split the non-significant effects. The most important distinction is between “no effect” and “unknown effect”. effectclass provides ggplot2 add-ons stat_effect() and scale_effect() to visualise the effects as points with shapes depending on the classification. It provides stat_fan() which displays the uncertainty as multiple overlapping intervals with different confidence probability. stat_fan() is inspired by Britton, E.; Fisher, P. &amp; J. Whitley (1998) More details on the package website: https://effectclass.netlify.com/ Britton, E.; Fisher, P. &amp; J. Whitley (1998). The Inflation Report Projections: Understanding the Fan Chart. Bank of England Quarterly Bulletin. "],
["posters.html", "1.2 Posters", " 1.2 Posters 1.2.1 A flexible dashboard for monitoring platform trials Alessio Crippa, Karolinska Institutet, postdoc Track(s): R Applications Abstract: The Data and Safety Monitoring Board (DSMB) is an essential component for a successful clinical trial. It consists of an independent group of experts that periodically revise and evaluate the accumulating data from an ongoing trial to assess patients’ safety, study progress, and drug efficacy. Based on their evaluation, a recommendation to continue, modify or stop the trial will be delivered to the trial’s sponsor. It is essential to provide the DSMB with the best delivery visualization tools for monitoring on a regular basis the live data from the study trial. We designed and developed an interactive dashboard using flexdashboard for R as a helping tool for assisting the DSMB in the evaluation of the results of the ProBio study, a clinical platform for improving treatment decision in patients with metastatic castrate resistant prostate cancer. We will focus on the customized structure for best displaying the most interesting variables and the adoption of interactive tools as a particularly useful aid for the assessment of the ongoing data. We will also cover the connection to the data sources, the automatic generation process, and the selected permission for the people in the DSMB to access the dashboard. Coauthor(s): Andrea Discacciati, Erin Gabriel, Martin Eklund . 1.2.2 PRDA package: Enhancing Statistical Inference via Prospective and Retrospective Design Analysis. Angela Andreella, University of Padua Track(s): R Life Sciences Abstract: There is a growing recognition of the importance of power analysis and calculation of the appropriate sample size when planning a research experiment. However, power analysis is not the only relevant aspect of the design of an experiment. Other inferential risks, such as the probability of estimating the effect in the wrong direction or the average overestimation of the actual effects, are also important. The evaluation of these inferential risks as well as the statistical power, in what Gelman and Carlin (2014) defined as Design Analysis, may help researchers to make informed choices both when planning an experiment or evaluating study results. We introduce the PRDA (Prospective and Retrospective Design Analysis) package that allows researchers to carry a Design Analysis under different experimental scenarios (Altoè et al., 2020). Considering a plausible effect size (or its prior distribution) researchers can evaluate either the inferential risks for given sample size or the required sampled size to obtain a given statistical power. Previously, PRDA functions were limited to mean differences between groups considering Cohen’s d in the Null significance Hypothesis Testing (NHST) framework. Now, we present the newly developed features that include other effect sizes (such as Pearson’s correlation) as well as Bayes Factor hypothesis testing. Coauthor(s): Vesely Anna, Zandonella Callegher Claudio, Pastore Massimiliano, Altoè Gianmarco . 1.2.3 Automate flexdashboard with GitHub Binod Jung Bogati, Data Analyst Intern at VIN Track(s): R Dataviz &amp; Shiny Abstract: flexdashboard is a great tool for building an interactive dashboard in R. We can host it for free on GitHub Pages, Rpubs and many other places. Hosted flexdashboard is static so changes in our data we need to manually update and publish every time. If we want to auto-update we may need to integrate Shiny. However, it may not be suitable for every case. To overcome this, we have a solution called GitHub Action. It’s a feature from GitHub which automates our tasks in a convenient way. With the help of GitHub Actions, we can automate our flexdashboard (Rmarkdown) updates. It builds a container that runs our R scripts. We can trigger it every time we push on GitHub or schedule it every X minutes/hours/days/month. If you want to learn more about the GitHub Action. And also know how to automate updates on your flexdashboard. Please do come and join me. 1.2.4 EasyReporting: a Bioconductor package for Reproducible Research implementation Dario Righelli, Department of Statistics, University of Padua, Post-Doc Track(s): R Applications, R Life Sciences Abstract: EasyReporting is a novel R/Bioconductor package for speeding up Reproducible Research (RR) implementation when analyzing data, implementing workflows or other packages. It is an S4 class helping developers to integrate an RR layer inside their software products, as well as data analysts speeding up their report production without learning the rmarkdown language. Thanks to minimal additional efforts by developers, the end-user has available a rmarkdown file within all the source code generated during the analysis, divided into Code Chunks (CC) ready for the compilation. Moreover, EasyReporting gives also the possibility to add natural language comments and textual descriptions into the final report to be compiled for producing an enriched document that incorporates input data, source code and output results. Once compiled, the final document can be attached to the publication of the analysis as supplementary material, helping the interested community to entirely reproduce the computational part of work. Despite other previously proposed solutions, that usually require a significant effort by the final user, potentially bringing him/her to renounce to include RR inside the scripts, our approach is versatile and easy to be incorporated, allowing to the final developer/analyzer to automatically create and store a rmarkdown document, and providing also methods for its compilation. Coauthor(s): angelini claudia . 1.2.5 NewWave: a scalable R package for the dimensionality reduction of single-cell RNA-seq Federico Agostinis, Università degli studi di Padova, Fellowship Track(s): R Life Sciences Abstract: The fast development of single cell sequencing technologies in the recent years has generated a gap between the throughput of the experiments and the capability of analizing the generated data. One recent method for dimensionality reduction of single-cell RNA-seq data is zinbwave, it uses zero inflated negative binomial likelihood function optimization to find biological meaningful latent factors and remove batch effect. Zinbwave has optimal performance but has some scalability issues due to large memory usage. To address this, we developed an R package with new software architec- ture extending zinbwave. In this package, we implement mini-batch stochastic gradient descent and the possibility of working with HDF5 files. We decide to use a negative binomial model following the observation that droplet sequencing technologies do not induce zero inflation in the data. Thanks to these improvements and the possi- bility of massively parallelize the estimation process using PSOCK clusters, we are able to speed up the computations with the same or even better results than zinbwave. This type of parallelization can be used on multiple hardware setups, ranging from simple laptops to dedicated server clusters. This, paired with the ability to work with out-of-memory data, enables us to analyze datasets with milions of cells. Coauthor(s): Chiara Romualdi, Gabriele Sales, Davide Risso . 1.2.6 orf: Ordered Random Forests Gabriel Okasa, Research Assistant and PhD Candidate at the Swiss Institute for Empirical Economic Research, University of St. Gallen, Switzerland Track(s): R Machine Learning &amp; Models Abstract: The R package ‘orf’ is a software implementation of the Ordered Forest estimator as developed in Lechner and Okasa (2019). The Ordered Forest flexibly estimates the conditional class probabilities of models involving categorical outcomes with an inherent ordering structure, known as ordered choice models. Additionally to common machine learning algorithms, the Ordered Forest enables estimation of marginal effects together with statistical inference and thus provides comparable output as in standard econometric models. Accordingly, the ‘orf’ package provides generic R functions to estimate, predict, plot, print and summarize the estimation output of the Ordered Forest along with various options for specific forest-related tuning parameters. Finally, computational speed is ensured as the core forest algorithm relies on the fast C++ forest implementation from the ranger package (Wright and Ziegler 2017). Coauthor(s): Michael Lechner . 1.2.7 Power Supply health status monitoring dashboard Marco Calderisi, Kode srl, CTO Track(s): R Dataviz &amp; Shiny Abstract: The Primis project dashboard allows to perform an analysis on the health status of power supplies boards on two levels: (1) analysis of a specific board, to check its status and the presence of any anomalies, (2) analysis of multiple boards within a single Power Supply, to check if the set of boards reveals abnormal behavior and if some boards behave in a distinctly different way from the others. The analysis algorithms and the web application were created using the programming language R, and in particular the Shiny library. The application is therefore divided into two parts that reflect these different types of analysis, called respectively “Product View” (analysis and diagnostics of a specific board) and “Product Comparison” (comparison analysis between multiple boards of the same Power Supply). Both analyzes can be carried out on an arbitrary time interval, selectable through a special application menu. The analysis is carried out by means of: (1) univariate analysis, focusing on a specific parameter of one or more channels and displaying aggregate information regarding the status of the board in the entire observation period (2) multivariate analysis, that is the application of multivariate algorithms that allows to perform an overall analysis of the board, taking into account all the variables simultaneously. Coauthor(s): Jacopo Baldacci, Caterina Giacomelli, Ilaria Ceppa, Davide Massidda, Matteo Papi, Gabriele Galatolo, Francesca Giorgolo, ferdinando giordano, alessandro iovene . 1.2.8 First-year ICT students dropout predicting with R models Natalja Maksimova, Virumaa College of Tallinn University of Technology, lecturer Track(s): R Machine Learning &amp; Models Abstract: The aim of this study is to find how it is possible to predict first-year ICT students dropout in one Estonian college, Virumaa College of Tallinn University of Technology (TalTech) and possibly to engage methods to decrease dropout rate. We perform three approaches of machine learning using R tools: logistic regressions, decision trees and Naive Bayes to predict. The models are computed on the basis of the TalTech study information system data. As a result, we propose a methodical approach that may be realized in practice at other institutions. All applied methods yield high prediction with more than 85% accuracies. In the same time some influencing and non-influencing factors were found in predicting ICT students’ dropout. Coauthor(s): Olga Dunajeva . 1.2.9 Benchmark Percentage Disjoint Data Splitting in Cross Validation for Assessing the Skill of Machine Olalekan Joseph Akintande, University of Ibadan, Ph.D. Student Track(s): R Machine Learning &amp; Models Abstract: The controversies surrounding dataset splitting technique and folklore of what has been or what should be, remain an open debate. Several authors (bloggers, researchers, and data scientists) in the field of machine learning and similar research areas, have proposed various arbitrary percentage disjoint dataset splitting (DDS) options for validating the skill of machine learning algorithms and by extension the appropriate percentage DDS based on cross-validation techniques. In this work, we propose benchmarks for which the percentage DDS procedure should be based. These benchmarks are founded on various training sizes (m) and serve as the basis and justification for the choice of an appropriate percentage DDS for assessing the skill of ML algorithms and related fields, on the concept of cross-validation techniques. Coauthor(s): O.E. Olubusoye . 1.2.10 Integrating professional software engineering practices in medical research software Patricia Ryser-Welch, Newcastle University, Population Health Science Institute, Research Associate, Track(s): R Applications Abstract: Health data sets are getting bigger, more complex, and are increasingly being linked up with other data sources. With this trend there is an increasing risk of patient identification and disclosure. Two different ways of mitigating this risk are to use a federated analysis approach or to use a data safe haven. DataSHIELD (www.datashield.ac.uk) is an established federated data analysis tool that is used in the medical sciences. This software has a variety of methods to reduce the risk of disclosure built in. Here we describe the steps we are taking to apply modern software engineering methodologies to DataSHIELD. The upcoming Medical Devices legislation requires that software has more rigourous testing done on it. While this legislation does not directly apply to software used for research, we think it is important the ideas behind this do filter down to research software. For us these principles include testing that functions work, as well as testing that they produce the correct answers. Using a static standard data set to test against (that is publicly available) is also an important aspect. This work is being done in a continuous integraion framework using Microsoft Azure. Additionally all our software is developed as open source. In addition to the protection DataSHIELD provides on its own we are also integrating it into our Trusted Research Environment as part of Connected Health Cities North East and North Cumbria. This will give an extra level of protection to data that may automatically flow from multiple data sources. Additionally, as analysis can be done in a federated way it means that that data does not need to leave its data controller’s environment. This opens up the possibility of analysis happening across trusts and regions. Coauthor(s): http://www.datashield.ac.uk . 1.2.11 Dealing with changing administrative boundaries: The case of Swiss municipalities Tobias Schieferdecker, Daily dealings with data at cynkra Track(s): R Applications Abstract: Switzerland’s municipalities are frequently merged or reorganized, in an attempt to reduce costs and increase efficiency. These mergers create a substantial problem for data analysis. Often it is desirable to study a municipality over time. But in order to create a time series for a region of interest, its borders should stay constant. Our goal is to provide R-functions that allow an easy and consistent handling of these mergers. We create a mapping table for municipality ID’s for a specified period of time, that allows us to track the mergers over time. We also provide weights, such as population as well as area of the municipalities, to facilitate the construction of weighted time series. Various other municipality mutations are also taken into account. We are creating two R packages: an infrastructure package that handles the task of keeping the data up to date; and a user package that contains the functions to deal with the mergers. Coauthor(s): Thomas Knecht, Kirill Müller, Christoph Sax . 1.2.12 badDEA: An R package for measuring firms’ efficiency adjusted by undesirable outputs Yann Desjeux, INRAE, France Track(s): R Applications Abstract: Growing concerns on the detrimental effects of human production activities on the environment, e.g. air, soil and water pollution, have triggered the development of new performance indicators (including productivity and efficiency measures) accounting for such undesirable impacts. Firms can now be benchmarked not only in terms of economic performance, but also in terms of environmental performance linked to production. In the performance benchmarking literature, and more specifically the one on the non-parametric approach Data Envelopment Analysis (DEA), several methodologies have been developed to consider these impacts as undesirable (or bad) outputs. Related empirical applications in the literature, performed with various software, show that conclusions differ depending on the way these undesirable outputs are introduced. However none of these methodologies are routinely developed in R. In this context, we developed the badDEA package in order to provide a consistent and single framework where users (students, researchers, practitioners) can find the major methodologies proposed in the literature to compute efficiency measures that are adjusted by undesirable outputs. In this presentation, we will describe the aim, structure and options of the badDEA package, unfolding all the methodologies in their different variants and providing a promising tool for decision-making. Coauthor(s): K Hervé DAKPO; Yann DESJEUX; Laure LATRUFFE . "],
["regular-talks.html", "1.3 Regular talks", " 1.3 Regular talks 1.3.1 Design Patterns For Big Shiny Apps Alex Gold, Solutions Engineer, RStudio Track(s): R Dataviz &amp; Shiny, R Production Abstract: In about 20 minutes on the morning of January 27, 2020, one engineer launched over 500 individual cloud server instances for workshop attendees at RStudio::conf and managed them for the duration of the workshops — all from a Shiny app. The RStudio team manages a variety of production systems using Shiny apps including our workshop infrastructure and access to our sales demo server. The Shiny apps are robust enough for these mission-critical activities because of an important lesson from web engineering: separation of concerns between front-end user interaction logic and back-end business logic. This design pattern can be implemented in R by creating user interfaces in Shiny and managing interactions with other systems with Plumber APIs and R6 classes. This pattern allows for even complex Shiny apps to still be understandable and maintainable. Moreover, this pattern of designing and building large Shiny apps is broadly applicable to any app that has substantial interaction with outside systems. Session attendees will gain an understanding of this pattern, which can be useful for many large Shiny apps. Coauthor(s): Cole Arendt . 1.3.2 Using XGBoost, Plumber and Docker in production to power a new banking product André Rivenæs, Data Scientist, PwC Markus Mortensen, PwC Track(s): R Machine Learning &amp; Models, R Production Abstract: Buffer is a brand new and innovative banking product by one of the largest retail banks in Norway, Sparebanken Vest, and it is powered by R. In fact, the product’s decision engine is written entirely in R. We analyze whether a customer should get a loan and how much loan they should be allocated by analyzing large amounts of data from various sources. An essential part is analyzing the customer’s invoices using machine learning (XGBoost). In this talk, we will cover: How we use ML and Bayesian statistics to estimate the probability of an invoice being repaid. How we successfully put the decision engine in production, using e.g. Plumber, Docker, CircleCI and Kubernetes. What we have learned from using R in production at scale. 1.3.3 Astronomical source detection and background separation: a Bayesian nonparametric approach Andrea Sottosanti, University of Padova Track(s): R Machine Learning &amp; Models, R Applications Abstract: We propose an innovative approach based on Bayesian nonparametric methods to the signal extraction of astronomical sources in gamma-ray count maps under the presence of a strong background contamination. Our model simultaneously induces clustering on the photons using their spatial information and gives an estimate of the number of sources, while separating them from the irregular signal of the background component that extends over the entire map. From a statistical perspective, the signal of the sources is modeled using a Dirichlet Process mixture, that allows to discover and locate a possible infinite number of clusters, while the background component is completely reconstructed using a new flexible Bayesian nonparametric model based on b-spline basis functions. The resultant can be then thought of as a hierarchical mixture of nonparametric mixtures for flexible clustering of highly contaminated signals. We provide also a Markov chain Monte Carlo algorithm to infer on the posterior distribution of the model parameters, and a suitable post-processing algorithm to quantify the information coming from the detected clusters. Results on different datasets confirm the capacity of the model to discover and locate the sources in the analysed map, to quantify their intensities and to estimate and account for the presence of the background contamination. Coauthor(s): Mauro Bernardi, Alessandra R. Brazzale, Roberto Trotta, David A. van Dyk . 1.3.4 Creating drag-and-drop shiny applications using sortable Andrie de Vries, Solutions engineer at RStudio, Author of “R for Dummies” Track(s): R Dataviz &amp; Shiny Abstract: Using the learnr package you can create interactive tutorials in your R Markdown documents. For a long time, you could only use the built-in question types, including R coding exercises and quizzes with single or multiple choice answers. Since the release of learnr version 0.10.0, it has been possible to create custom question types. The new framework allows you to define different behaviour for the appearance and behaviour of your questions. The sortable package uses this capability to introduce new question types for ranking questions and bucketing questions. With ranking questions you can ask your students to arrange answer options in the correct order. With bucketing questions you can ask them to arrange answer options into different buckets. The sortable package achieves this by exposing an htmlwidget wrapper around the SortableJS JavaScript library. This library lets you sort any object in a Shiny app, with dynamic drag-and-drop behaviour. For example, you can arrange items in a list, or drag-and-drop the order of shiny tabs. During this presentation you will see how easy it is to add dynamic behaviour to your shiny app, and how simple it is to use the new sorting and bucketing tasks in your tutorials. Coauthor(s): Barret Schloerke, Kenton Russell . 1.3.5 High dimensional sampling and volume computation Apostolos Chalkis, PhD in Computer Science Track(s): R Machine Learning &amp; Models Abstract: Sampling from multivariate distributions is a fundamental problem in statistics that plays important role in modern machine learning and data science. Many important problems such as convex optimization and multivariate integration can be efficiently solved via sampling. This talk presents the CRAN package volesti which offers to R community efficient C++ implementations of state-of-the-art algorithms for sampling and volume computation of convex sets. It scales up to hundred or thousand dimensions, depending the problem, providing the most efficient implementations for sampling and volume computation to date. Thus, volesti allows users to solve problems in dimensions and order of magnitude higher than before. We present the basic functionality of volesti and show how it can be used to provide approximate solutions to intractable problems in combinatorics, financial modeling, bioinformatics and engineering. We stand out two famous applications in finance. We show how volesti can be used to detect financial crises and evaluate portfolios performance in large stock markets with hundreds of assets, by giving real life examples using public data. Coauthor(s): Vissarion Fisikopoulos . 1.3.6 Fake News: AI on the battle ground Ayomide Shodipo, Senior Developer Advocate &amp; Media Developer Expert at Cloudinary Track(s): R Machine Learning &amp; Models, R Life Sciences, R Production, R World Abstract: Assumed products have been a longstanding and growing pain for companies around the globe. In addition to impacting company revenue, they damage brand reputation and customer confidence. Companies were asked to build a solution for a global electronics brand that can identify fake products by just taking one picture on a smartphone. In this session, we will look into the building blocks that make this AI solution work. We’ll find out that there is much more to it than just training a convolutional neural network. We look at challenges like how to manage and monitor the AI model and how to build and improve the model in a way that fits your DevOps production chain. Learn how we used Azure Functions, Cosmos DB and Docker to build a solid foundation. See how we used the Azure Machine Learning service to train the models. And find out how we used Azure DevOps to control, build and deploy this state-of-the-art solution. 1.3.7 From consulting to open-source and back Christoph Sax, R-enthusiast, economist @cynkra Track(s): R World Abstract: Open-source development is a great source of satisfaction and fulfillment, but someone has to pay the bills. A straightforward solution is to consult customers and help them to pick the right tools. As a small group of R enthusiasts, we try to align open source development by supporting our clients to accomplish their goals, contributing to the community along the way. It turns out that the benefits work in both ways: In addition to funding, consulting work allows us to test our tools and to improve their usability in a practical setting. At the same time, the involvement in open source development sharpens our analytical skills and serves as a first stop for new customers. Ideally, consulting projects lead to new developments, which in turn lead to new consulting projects. Coauthor(s): Kirill Müller . 1.3.8 Deduplicating real estate ads using Naive Bayes record linkage Daniel Meister, Datahouse AG Track(s): R Applications Abstract: We demonstrate in this talk, how we used a containerized R and PostgreSQL data pipeline to deduplicate 60 million real estate ads from Germany and Switzerland using a multi-step Naive Bayes record linkage approach. Real estate platforms publish millions of rental flat and condominium ads yearly. A given region or country of interest is normally covered by various competing platforms, leading to multiple published ads for a single real world object. Because quantifying and modeling the real estate market requires unbiased input data, our aim was to deduplicate real estate ads using Naive Bayes record linkage. We used commercially available German and Swiss real estate ad data from 2012 to 2019 consisting of approximately 60 million individual records. After multiple data cleaning and preparation steps we employed a Naive Bayes weighting of 12-14 variables to calculate similarity scores between ads and determined a linkage threshold based on expert judgment. The deduplication pipeline consisted of three steps: linking ads based on identity comparisons, linking similar ads within small regional areas (municipalities) and linking similar ads within large regional areas (cantons, states). The pipeline was deployed as a containerized setup with in-memory calculations in R and out-of-memory calculations and data storage in PostgreSQL. Deduplication linked the around 60 million ads to around 14 million object groups (Germany: 10 millions, Switzerland: 4 millions). The distribution of similarity scores showed high separation power and the resulting object groups displayed high homogeneity in geographic location and price distribution. Furthermore, yearly results corresponded well with published relocation rates. Using Naive Bayes record linkage to deduplicate real estate ads resulted in a sensible grouping of ads into object groups (rental flats, condominiums). We were able to combine similarities across different variables into a single similarity score. An advantage of the Naive Bayes approach is the high interpretability of the influence of individual variables. However, by manually determining the linkage threshold our results are heavily influenced by possible expert biases. The containerized R and PostgreSQL setup proved it’s portability and scaling capabilities. The same approach could easily be transferred to other domains requiring deduplication of multivariate data sets. 1.3.9 {polite}: web etiquette for R users Dmytro Perepolkin, Lund University Track(s): R World, R Applications Abstract: Data is everywhere, but it does not mean it is freely available. What are best practices and acceptable norms for accessing the data on the web? How does one know when it is OK to scrape the content of a website and how to do it in such a way that it does not create problems for data owner and/or other users? This talk with provide examples of using {polite} package for safe and responsible web scraping. The three pillars of {polite} are seeking permission, taking slowly and never asking twice. 1.3.10 Hydrological Modelling and R Emanuele Cordano, www.rendena100.eu Track(s): R Applications Abstract: Eco-hydrological and biophysical models are increasingly used in the contexts of hydrology, ecology, precision agriculture for better management of water resources and climate change impact studies at various scales: local, watershed or regional scale. However, to satisfy the researchers and stakeholders demand, user friendly interfaces are needed. The integration of such models in the powerful software environment of R greatly eases the application, input data preparation, output elaboration and visualization. In this work we present new developments for a R interface (R open-source package geotopbricks (https://CRAN.R-project.org/package=geotopbricks) and related) for the GEOtop hydrological distributed model (www.geotop.org - GNU General Public License v3.0). This package aims to be a link between the work of environmental engineers, who develop hydrological models, and the ones of data and applied scientists, who can extract information from the model results. Applications related to the simulation of water cycle dynamics (model calibration, mapping, data visualization) in some alpine basins and under scenarios of climate change and variability are shown. In particular, we will present an application to predict with the model winter snow conditions, which play a critical role in governing the spatial distribution of fauna in temperate ecosystems. Coauthor(s): Giacomo Bertoldi . 1.3.11 GeneTonic: enjoy RNA-seq data analysis, responsibly Federico Marini, Center for Thrombosis and Hemostasis (CTH) &amp; Institute of Medical Biostatistics, Epidemiology and Informatics (IMBEI) - University Medical Center Mainz Track(s): R Life Sciences Abstract: Interpreting the results from RNA-seq transcriptome experiments can be a complex task, where the essential information is distributed among different tabular and list formats - normalized expression values, results from differential expression analysis, and results from functional enrichment analyses. The identification of relevant functional patterns, as well as their contextualization in the data and results at hand, are not straightforward operations if these pieces of information are not combined together efficiently. Interactivity can play an essential role in simplifying the way how one accesses and digests RNA-seq data analysis in a more comprehensive way. I introduce GeneTonic (https://github.com/federicomarini/GeneTonic), an application developed in Shiny and based on many essential elements of the Bioconductor project, that aims to reduce the barrier to understanding such data better, and to efficiently combine the different components of the analytic workflow. For example, starting from bird’s eye perspective summaries (with interactive bipartite gene-geneset graphs, or enrichment maps), it is easy to generate a number of visualizations, where drill-down user actions enable further insight and deliver additional information (e.g., gene info boxes, geneset summary, and signature heatmaps). Complex datasets interpretation can be wrapped up into a single call to the GeneTonic main function, which also supports built-in RMarkdown reporting, to both conclude an exploration session, or also to generate in batch the output of the available functionality, delivering an essential foundation for computational reproducibility. 1.3.12 A simple and flexible inactivity/sleep detection R package Francesca Giorgolo, Kode s.r.l. - Data Scientist Track(s): R Life Sciences Abstract: With the widespread usage of wearable devices great amount of data became available and new fields of application arised, like health monitoring and activity detection. Our work focused on inactivity and sleep detection from continuous raw tri-axis accelerometer data, recorded using different accelerometers brands having sampling frequencies below and above 1Hz. The algorithm implemented is the SPT-window detection algorithm described in literature slighty modified to met the flexibility requirement we imposed ourselves. The R package developed provides functions to clean data, to identify inactivity/sleep windows and to visualize the results. The main function has a parameter to specify the measurement unit of the data, a threshold to distinguish low and high activity and also a parameter to handle non-wearing periods, where a non wear period is defined as a period of time where all the accelerometers are equal to zero. Other functions allow to separate overlapped accelerometer signals, i.e. when a device is replaced by another, and to visualize the obtained results. Coauthor(s): Ilaria Ceppa, Marco Calderisi, Davide Massidda, Matteo Papi, Gabriele Galatolo, Andrea Spinelli, Andrea Zedda, Jacopo Baldacci, Caterina Giacomelli . 1.3.13 progressr: An Inclusive, Unifying API for Progress Updates Henrik Bengtsson, UCSF, Assoc Prof, CS/Stats, R since 2000 Track(s): R Production, R Applications Abstract: The ‘progressr’ package provides a minimal, unifying API for scripts and packages to report progress from anywhere including when using parallel processing to anywhere. It is designed such that the developer can focus on what to report progress on without having to worry about how to present it. The end user has full control of how, where, and when to render these progress updates. Progress bars from popular progress packages are supported and more can be added. The ‘progressr’ is inclusive by design. Specifically, no assumptions are made how progress is reported, i.e. it does not have to be a progress bar in the terminal. Progress can also be reported as audio (e.g. unique begin and end sounds with intermediate non-intrusive step sounds), or via a local or online notification system. Another novelty is that progress updates are controlled and signaled via R’s condition framework. Because of this, there is no need for progress-specific arguments and progress can be reported from nearly everywhere in R, e.g. in classical for and while loops, within map-reduce APIs like the ‘lapply()’ family of functions, ‘purrr’, ‘plyr’, and ‘foreach’. It also works with parallel processing via the ‘future’ framework, e.g. ‘future.apply’, ‘furrr’, and ‘foreach’ with ‘doFuture’. The package is compatible with Shiny applications. 1.3.14 varycoef: Modeling Spatially Varying Coefficients Jakob Dambon, HSLU &amp; UZH, Switzerland Track(s): R Machine Learning &amp; Models Abstract: In regression models for spatial data, it is often assumed that the marginal effects of covariates on the response are constant over space. In practice, this assumption might often be questionable. Spatially varying coefficient (SVC) models are commonly used to account for spatial structure within the coefficients. With the R package varycoef, we provide the frame work to estimate Gaussian process-based SVC models. It is based on maximum likelihood estimation (MLE) and in contrast to existing model-based approaches, our method scales better to data where both the number of spatial points is large and the number of spatially varying covariates is moderately-sized, e.g., above ten. We compare our methodology to existing methods such as a Bayesian approach using the stochastic partial differential equation (SPDE) link, geographically weighted regression (GWR), and eigenvector spatial filtering (ESF) in both a simulation study and an application where the goal is to predict prices of real estate apartments in Switzerland. The results from both the simulation study and application show that our proposed approach results in increased predictive accuracy and more precise estimates. Coauthor(s): Fabio Sigrist, Reinhard Furrer . 1.3.15 FastAI in R: preserving wildlife with computer vision Jędrzej Świeżewski, Data Scientist at Appsilon Track(s): R Machine Learning &amp; Models Abstract: In this presentation, we will discuss using the latest techniques in computer vision as an important part of “AI for Good” efforts, namely, enhancing wildlife preservation. We will present how to make use of the latest technical advancements in an R setup even if they are originally implemented in Python. A topic rightfully receiving growing attention among Machine Learning researchers and practitioners is how to make good use of the power obtained with the advancement of the tools. One of the avenues in these efforts is assisting wildlife conservation by employing computer vision in making observations of wildlife much more effective. We will discuss several of such efforts during the talk. One of the very promising frameworks for computer vision developed recently is the Fast.ai wrapper of PyTorch, a Python framework used for computer vision among other things. While it incorporates the latest theoretical developments in the field (such as one cycle policy training) it provides an easy to use framework allowing a much wider audience to benefit from the tools, such as AI for Good initiatives run by people who are not formally trained in Machine Learning. During the presentation we will show how to make use of a model trained using the Python’s fastai library within an R workflow with the use of the reticulate package. We will focus on use cases concerning classifying species of African wildlife based on images from camera traps. Coauthor(s): Marek Rogala . 1.3.16 The R Consortium 2020: adapting to rapid change and global crisis Joseph Rickert, RStudio: R Community Ambassador, R Consortium’s Board of Directors Track(s): R World Abstract: The COVID-19 pandemic has turned the world upside down, and like everyone else the R Community is learning how to adapt to rapid change in order to carry on important work while looking for ways to contribute to the fight against the pandemic. In this talk, I will report on continuing R Community work being organized through the R Consortium such as the R Hub, R User Group Support Program and Diversity and Inclusion Projects; and through the various working groups including the Validation Hub, R / Pharma, R / Medicine and R Business. Additionally, I will describe some of the recently funded ISC projects and report on the COVID-19 Data Forum, a new project that the R Consortium is organizing in partnership with Stanford’s Data Science Institute. 1.3.17 Powering Turing e-Atlas with R Layik Hama, Alan Turing Institute Track(s): R Applications, R Production, R Dataviz &amp; Shiny Abstract: Turing e-Atlas is a research project under the Urban Analytics research theme at Alan Turing Institute (ATI). The ATI is UK’s national institute for data science and Artificial Intelligence based at the British Library in London. The research is a grand vision for which we have been trying to take baby steps under the banner of an e-Atlas. And we believe R is positioned to play a foundation role in any scalable solution to analyse and visualize large scale datasets especially geospatial datasets. The application presented is built using RStudio’s Plumber package which relies on solid libraries to develop web applications. The front-end is made up of Uber’s various visualization packages using Facebook’s React JavaScript framework. Coauthor(s): Dr Nik Lomax, Dr Roger Beecham . 1.3.18 Using process mining principles to extract a collaboration graph from a version control system log Leen JOOKEN, Hasselt University, PhD student Track(s): R Production, R World Abstract: Knowledge management is an indispensable component of modern-day, fast changing and flexible software engineering environments. A clear overview on how software developers collaborate can reveal interesting insights such as the general structure of collaboration, crucial resources, and risks in terms of knowledge preservation that can arise when a programmer decides to leave the company. Version control system (VCS) logs, which keep track of what team members work on and when, contain data to provide these insights. We present an R package that provides an algorithm which extracts and visualizes a collaboration graph from VCS log data. The algorithm is based on principles from graph theory, cartography and process mining. Its structure consists of four phases: (1) building the base graph, (2) calculating weights for nodes and edges, (3) simplifying the graph using aggregation and abstraction, and (4) extending it to include specific insights of interest. Each of these phases offers the user a lot of flexibility in deciding which parameters and metrics to include. This makes it possible for the human expert to exploit his existing knowledge about the project and team to guide the algorithm in building the graph that best fits his specific use case, and hence will provide the most accurate insights. Coauthor(s): Gert Janssenswillen, Mathijs Creemers, Mieke Jans, Benoît Depaire . 1.3.19 Manifoldgstat: an R package for spatial statistics of manifold data Luca Torriani, MOX, Department of Mathematics, Politecnico di Milano Ilaria Sartori, Politecnico di Milano Track(s): R Machine Learning &amp; Models Abstract: The statistical analysis of data belonging to Riemannian manifolds is becoming increasingly important in many applications, such as shape analysis or diffusion tensor imaging. In many cases, the available data are georeferenced, making spatial dependence a non-negligible data characteristic. Modeling and accounting for it, typically, is not trivial, because of the non-linear geometry of the manifold. In this contribution, we present the Manifoldgstat R package, which provides a set of fast routines allowing to efficiently analyze sets of spatial Riemannian data, based on state-of-the-art statistical methodologies. The package stems from the need to create an efficient and reproducible environment allowing to run extensive simulation studies and bagging algorithms for spatial prediction of symmetric positive definite matrices. The package implements three main algorithms (Pigoli et al, 2016, Menafoglio et al, 2019, Sartori &amp; Torriani, 2019). The latter two are particularly computationally demanding, as they rely on Random Domain Decompositions of the geographical domain. To substantially improve performances, the package exploits dplyr and Rcpp to integrate R with C++ code, where template factories handle all run-time choices. In this communication, we shall revise the characteristics of the three methodologies considered, and the key-points of their implementation. Coauthor(s): Alessandra Menafoglio, Piercesare Secchi . 1.3.20 Voronoi Linkage for Spatially Misaligned Data Luís G. Silva e Silva, Food and Agriculture Organization - FAO - Data Scientist Track(s): R Dataviz &amp; Shiny, R World Abstract: In studies of elections, voting outcomes are point-referenced at voting stations while socioeconomic covariates are areal data available at census tracts. The misaligned spatial structure of these two data sources makes the regression analysis to identify socioeconomic factors that affect the voting outcomes a challenge. Here we propose a novel approach to link these two sources of spatial data through Voronoi tessellation. Our proposal is creating a Voronoi tessellation with respect to the point-referenced data, with this outset, the spatial points become a set of mutually exclusive polygons named Voronoi cells. The extraction of data from the census tracts is proportional to the intersection area of each census tract polygon and Voronoi cells. Consequently, we use 100% of the available information and preserve the polygons’ autocorrelation structure. When visualised through our Shiny App, the method provides a finer spatial resolution than municipalities and facilitates the identification of spatial structures at a most detailed level. The technique is applied for the 2018 Brazilian presidential election data. The tool provides deep access to Brazilian election results by enabling to create general maps, plots, and tables by states and cities. Coauthor(s): Lucas Godoy, Douglas Azevedo, Augusto Marcolin, Jun Yan . 1.3.21 Be proud of your code! Tools and patterns for making production-ready, clean R code Marcin Dubel, Software Engineer at Appsilon Data Science Track(s): R Production, R World Abstract: In this talk you’ll learn the tools and best practices for making clean, reproducible R code in a working environment ready to be shared and productionalised. Save your time for maintenance, adjusting and struggling with packages. R is a great tool for fast data analysis. It’s simplicity in setup combined with powerful features and community support makes it a perfect language for many subject matter experts e.g. in finance or bioinformatics. Yet often what started as a pet project or proof of concept begins to grow and expand, with additional collaborators working on it. It is then crucial that you have your project organised well, reusable, with an environment set, so that the code works every time and on any machine. Otherwise the solution won’t be used by anyone but you. By following a few patterns and with appropriate tools it won’t be overwhelming or disturbing and will highlight the true value of the code. Both Appsilon and I personally have taken part in many R projects for which the goal was to clean and organise the code as well as the project structure. We would like to share our experience, best practices and useful tools to share code shamelessly. During the presentation I will show: setting up the development environment with packrat, renv and docker, organising the project structure, the best practices in writing R code, automated with linter, sharing the code using git, organising workflow with drake, optimising the Shiny apps and data loading with plumber and database, preparing the tests and continuous integration circle CI. 1.3.22 Going in the fast lane with R. How we use R within the biggest digital dealer program in EMEA. Marco Cavaliere, Like Reply - Business Data Analyst Track(s): R Production, R Machine Learning &amp; Models Abstract: How we are using R as the foundation of all the data-related tasks in the biggest dealer digital program at FCA. From simple tasks as dashboarding or reporting to more strategic capabilities as predicting advertising ROI through Tensorflow or developing a production-grade, data-driven microservices, we leverage R ecosystem to deliver better results and increase the data awareness for all the project stakeholders. 1.3.23 R alongside Airflow, Docker and Gitlab CI Matthias Bannert, Research Engineering Lead at ETH Zurich, KOF Swiss Economic Institute Track(s): R Production Abstract: The KOF Swiss Economic Institute at ETH Zurich (Switzerland) regularly surveys more than 10K companies, computes economic indicators and forecasts as it monitors the national economy. Today, the institute updates its website in automated fashion and operates automated data pipelines to partners such as regional statistical offices or the Swiss National Bank. At KOF, production is based on an open source ecosystem to a large degree. More and more processes are being migrated to an environment that consists of open source components Apache Airflow, Docker, Gitlab Continous Integration, PostgreSQL and R. This talk shows not only how well R interfaces and works with all parts from workflow automation to databases, but also how R’s advantages impact this system: From R Studio Servers to internal packages and an own internal mini-CRAN, the use of the R language is crucial in making the environment stable and convenient to maintain with the software carpentry type of background. 1.3.24 DaMiRseq 2.0: from high dimensional data to cost-effective reliable prediction models Mattia Chiesa, Senior data scientist @ Centro Cardiologico Monzino IRCCS Track(s): R Life Sciences Abstract: High dimensional data generated by modern high-throughput platforms pose a great challenge in selecting a small number of informative variables, for biomarker discovery and classification. Machine learning is an appropriate approach to derive general knowledge from data, identifying highly discriminative features and building accurate prediction models. To this end, we developed the R/Bioconductor package DaMiRseq, which (i) helps researchers to filter and normalize high dimensional datasets, arising from RNA-Seq experiments, by removing noise and bias and (ii) exploits a custom machine learning workflow to select the minimum set of robust informative features able to discriminate classes. Here, we present the version 2.0 of the DaMiRseq package, an extension that provides a flexible and convenient framework for managing high dimensional data such as omics data, large-scale medical histories, or even social media and financial data. Specifically, DaMiRseq 2.0 implements new functions that allow training and testing of several different classifiers and selection of the most reliable one, in terms of classification performance and number of selected features. The resulting classification model can be further used for any prediction purpose. This framework will give users the ability to build an efficient prediction model that can be easily replicated in further related settings. Coauthor(s): Chiara Vavassori, Gualtiero I. Colombo, Luca Piacentini . 1.3.25 How to apply R in a hospital environment on standard available hospital-wide data Mieke Deschepper, University hospital Ghent, staf member Strategic Policy cell, Ph.D. Track(s): R Life Sciences Abstract: Lots of data is registered within hospitals, for financial, clinical and administrative purposes. Today, this data is barely used. Due to not knowing the existence of the data, the possible applications and the skills to execute the analysis, … In this presentation we show how we can apply R on this data and what the possibilities are using standard available hospital-wide data on a low cost budget. 1. Reporting with R - using R and markdown as a tool for management reporting - Using R for data handling (ETL) - Shiny applications as alternative for dashboarding 2. Using R as a statistical tool - Performing regression models to gain insight in certain predictor 3. Using R a data science tool - Using R to perform Machine Learning analysis, e.g. Random Forests - Using R for the data wrangling and handle the high dimensional data 4. Requirements for all of the above 1.3.26 Computer Algebra Systems in R Mikkel Meyer Andersen, Assoc. Prof., Department of Mathematical Sciences, Aalborg University, Denmark Track(s): R World, R Machine Learning &amp; Models, R Applications Abstract: R’s ability to do symbolic mathematics is largely restricted to finding derivatives. There are many tasks involving symbolic math and that are of interest to R users, e.g. inversion of symbolic matrices, limits and solving non-linear equations. Users must resort to other computer algebra systems (CAS) for such tasks and many R users (especially outside of academia) do not readily have access to such software. There are also other indirect use-cases of symbolic mathematics in R that can exploit other strengths of R, including Shiny apps with auto-generated mathematics exercises. We are maintaining two packages enabling symbolic mathematics in R: Ryacas and caracas. Ryacas is based on Yacas (Yet Another Computer Algebra System) and caracas is based on SymPy (Python library). Each have their advantages: Yacas is extensible and has a close integration to R which makes auto-generated mathematics exercises easy to make. SymPy is feature-rich and thus gives many possibilities. In this talk we will discuss the two packages and demonstrate various use-cases including uses that help understanding statistical models and Shiny apps with auto-generated mathematics exercises. Coauthor(s): Søren Højsgaard . 1.3.27 Interpretable and accessible Deep Learning for omics data with R and friends Moritz Hess, Research Associate, Institute of Medical Biometry and Statistics, Faculty of Medicine and Medical Center - University of Freiburg Track(s): R Life Sciences Abstract: Recently, generative Deep Learning approaches were shown to have a huge potential for e.g. retrieving compact, latent representations of high-dimensional omics data such as single-cell RNA-Seq data. However, there are no established methods to infer how these latent representations relate to the observed variables, i.e. the genes. For extracting interpretable patterns from gene expression data that indicate distinct sub-populations in the data, we here employ log-linear models, applied to the synthetic data and corresponding latent representations, sampled from generative deep models, which were trained with single-cell gene expression data. While omics data are routinely analyzed in R and powerful toolboxes, tailored to omics data are available, there are no established and truely accessible approaches for Deep Learning applications here. To close this gap, we here demonstrate how easily customizable Deep Learning frameworks, developed for the Julia programming language, can be leveraged in R, to perform accessible and interpretable Deep Learning with omics data. Coauthor(s): Stefan Lenz, Harald Binder . 1.3.28 Elevating shiny module with {tidymodules} Mustapha Larbaoui, Novartis, Associate Director, Scientific Computing &amp; Consulting Track(s): R Dataviz &amp; Shiny Abstract: Shiny App developers have warmly welcomed the concept of Shiny modules as a way to simplify the app development process through the introduction of reusable building blocks. Shiny modules are similar in spirit to the concept of functions in R, except each is implemented with paired ui and server codes along with their own namespace. The {tidymodules} R package introduces a novel structure that harmonizes module development based on R6 (https://r6.r-lib.org/), which is an implementation of encapsulated object-oriented programming for R, thus knowledge of R6 is a prerequisite for using {tidymodules} to develop Shiny modules. Some key features of this package are module encapsulation, reference semantics, central module store and an innovative framework for enabling and facilitating cross module – module communication. It does this through the creation of “ports”, both input and output, where users may pass data and information through pipe operators. Because the connections are strictly specified, the module network may be visualized which shows how data move from one module to another. We feel the {tidymodules} framework will simplify the module development process and will reduce the code complexity through programming concepts like inheritance. Coauthor(s): Doug Robinson, Xiao Ni, David Granjon . 1.3.29 APFr: Average Power Function and Bayes FDR for Robust Brain Networks Construction Nicolo’ Margaritella, University of Edinburgh Track(s): R Life Sciences Abstract: Brain functional connectivity is widely investigated in neuroscience. In recent years, the study of brain connectivity has been largely aided by graph theory. The link between time series recorded at multiple locations in the brain and a graph is usually an adjacency matrix. This converts a measure of the connectivity between two time series, typically a correlation coefficient, into a binary choice on whether the two brain locations are functionally connected or not. As a result, the choice of a threshold over the correlation coefficient is key. In the present work, we propose a multiple testing approach to the choice of a suitable threshold using the Bayes false discovery rate (FDR) and a new estimator of the statistical power called average power function (APF) to balance the two types of statistical error. We show that the proposed APF behaves well in case of independence of the tests and it is reliable under several dependence conditions. Moreover, we propose a robust method for threshold selection using the 5% and 95% percentiles of APF and FDR bootstrap distributions, respectively, to improve stability. In addition, we developed a R-package called APFr which performs APF and Bayes FDR robust estimation and provides simple examples to improve usability. The package has attracted more than 3200 downloads since its publication online (June 2019) at https://CRAN.R-project.org/package=APFr. Coauthor(s): Piero Quatto . 1.3.30 Flexible Meta-Analysis of Generalized Additive Models with metagam Øystein Sørensen, Associate Professor, University of Oslo Track(s): R Life Sciences, R Machine Learning &amp; Models Abstract: Analyzing biomedical data from multiple studies has great potential in terms of increasing statistical power, enabling detection of associations of smaller magnitude than would be possible analyzing each study separately. Restrictions due to privacy or proprietary data as well as more practical concerns can make it hard to share datasets, such that analyzing all data in a single mega-analysis might not be possible. Meta-analytic methods provide a way to overcome this issue, by combining aggregated quantities like model parameters or risk ratios. However, most meta-analytic tools have focused on parametric statistical models, and software for meta-analyzing semi-parametric models like generalized additive models (GAMs) have not been developed. The metagam package attempts to fill this gap: It provides functionality for removing individual participant data from GAM objects such that they can be analyzed in a common location; furthermore metagam enables meta-analysis of the resulting GAM objects, as well as various tools for visualization and statistical analysis. This talk will illustrate use of the metagam package for analysis of the relationship between sleep quality and brain structure using data from six European brain imaging cohorts. Coauthor(s): Andreas Brandmaier . 1.3.31 EPIMOD: A computational framework for studying epidemiological systems. Paolo Castagno, Ph.D. Track(s): R Life Sciences, R Applications Abstract: Computational-mathematical models can be efficiently used to provide new insights into drivers of a disease spread, investigating different explanations of observed resurgence and predicting potential effects of different therapies. In this context, we present a new general modeling framework for the analysis of epidemiological and biological systems, characterized by features that make easy its utilization even by researchers without advanced mathematical and computational skills. The implementation of the R package, called “Epimod”, provides a friendly interface to access the model creation and the analysis techniques implemented in the framework. In details, by exploiting the graphical formalism of the Petri Net it is possible to simplify the model creation phase, providing a compact graphical description of the system and an automatically derivation of the underlying stochastic or deterministic process. Then, by using four functions it is possible to deal with Model Generation, Sensitivity Analysis, Model Calibration and Model Analysis phases. Finally, the Docker containerization of all analysis techniques grants a high level of portability and reproducibility. We applied Epimod to model pertussis epidemiology, investigating alternative explanations of its resurgence and to predict potential effects of different vaccination strategies. Coauthor(s): Simone Pernice, Matteo Sereno, Marco Beccuti. . 1.3.32 CorrelAidX - Building R-focused Communities for Social Good on the Local Level Regina Siegers, CorrelAidX Coordination Track(s): R World Abstract: Data scientists with their valuable skills have enormous potential to contribute to the social good. This is also true for the R community - and R users seem to be especially motivated to use their skills for the social good, as the overwhelmingly positive reception of Julien Cornebise’s keynote “AI for Good” at useR2019 (Cornebise 2019) has shown. However, specific strategies for putting the abstract goal “use data science for the social good” into practice are often missing, especially in volunteering contexts like the R community, where resources are often limited. In our talk, we present formats that we have implemented on the local level to build R-focused, data-for-good communities across Europe. Originating from the German data4good network CorrelAid with its over 1600 members, we have established 9 local CorrelAidX groups in Germany, the Netherlands, and France. The specific formats build on a three-pillared concept of community building, namely group-bonding, social entrepreneurship and outreach. We present multiple examples that illustrate how our local chapters operate to put data science for good into practice - using the formats of data dialogues, local data4good projects, and CorrelAidX workshops. Lastly, we also outline possibilities to implement such formats in cooperation between CorrelAidX chapters and R community groups such as R user groups or RLadies chapters. Coauthor(s): Konstantin Gavras . 1.3.33 Interactive visualization of complex texts Renate Delucchi Danhier, Post-Doc, TU Dortmund Track(s): R Dataviz &amp; Shiny Abstract: Hundreds of speakers may describe the same circumstance - e.g. explaining a fixed route to a goal - without producing two identical texts. The enormous variability of language and the complexity involved in encoding meaning poses a real difficulty for linguists analyzing text databases. In order to aid linguists in identifying patterns to perform comparative research, we developed an interactive shiny app that enables quantitative analysis of text corpora without oversimplifying the structure of language. Route directions are an example of complex texts, in which speakers take cognitive decisions such as segmenting the route, selecting landmarks and organizing spatial concepts into sentences. In the data visualization, symbols and colors representing linguistic concepts are plotted into coordinates that relate the information to fixed points along the route. Six interconnected layers of meaning represent the multi-layered form-to-meaning mapping characteristic of language. The shiny app allows to select and deselect information on these different layers, offering a holistic linguistic analysis way beyond the complexity attempted within traditional linguistics. The result is a kind of visual language in itself that deconstructs the interconnected layers of meaning found in natural language. Coauthor(s): Paula González Ávalos . 1.3.34 CONNECTOR: a computational approach to study intratumor heterogeneity. Simone Pernice, Ph.D student at Department of Computer Science of the University of Turin Track(s): R Life Sciences Abstract: Literature is characterized by a broad class of mathematical models which can be used for fitting cancer growth time series, but with no a global consensus or biological evidence that can drive the choice of the correct model. The conventional perception is that the mechanistic models enable the biological understanding of the systems under study. However, there is no way that these models can capture the variability characterizing the cancer progression, especially because of the irregularity and sparsity of the available data. For this reason, we propose CONNECTOR, an R package built on the model-based approach for clustering functional data. Such method is based on the clustering and fitting of the data through a combination of cubic natural splines basis with coefficients treated as random variables. Our approach is particularly effective when the observations are sparse and irregularly spaced, as growth curves usually are. CONNECTOR provides a tool set to easily guide through the parameters selection, i.e., (i) the dimension of the spline basis, (ii) the dimension of the mean space and (iii) the number of clusters to fit, to be properly chosen before fitting. The effectiveness of CONNECTOR is evaluated on growth curves of Patient Derived Xenografts (PDXs) of ovarian cancer. Genomic analyses of PDXs allowed correlating fitted and clustered PDX growth curves to cell population distribution. Coauthor(s): Beccuti Marco, Sirovich Roberta, Cordero Francesca . 1.3.35 gWQS: An R Package for Linear and Generalized Weighted Quantile Sum (WQS) Regression Stefano Renzetti, PhD Student at Università degli Studi di Milano Track(s): R Machine Learning &amp; Models, R Life Sciences Abstract: Weighted Quantile Sum (WQS) regression is a statistical model for multivariate regression in high-dimensional datasets commonly encountered in environmental exposures. The model constructs a weighted index estimating the mixture effect associated with all predictor variables on an outcome. The package gWQS extends WQS regression to applications with continuous, categorical and count outcomes. We provide four examples to illustrate the usage of the package. Coauthor(s): Chris Gennings, Paul C. Curtin . 1.3.36 Transparent Journalism Through the Power of R Tatjana Kecojevic, SisterAnalyst.org; founder and director Track(s): R World Abstract: This study examines the often-tricky process of delivering data literacy programmes to professionals with most to gain from a deeper understanding of data analysis. As such, the author discusses the process of building and delivering training strategies to journalists in regions where press freedom is constrained by numerous factors, not least of all institutionalised corruption. Reporting stories that are supplemented with transparent procedural systems are less likely to be contradicted and challenged by vested interest actors. Journalists are able to present findings supported by charts and info graphics, but these are open to translation. Therefore, most importantly, the data and code of the applied analytical methodology should also be available for scrutiny and is less likely to be subverted or prohibited. As part of creating an accessible programme geared to acquiring skills necessary for data journalism, the author takes a step-by-step approach to discussing the actualities of building online platforms for training purposes. Through the use of grammar of graphics in R and Shiny, a web application framework for R, it is possible to develop interactive applications for graphical data visualisation. Presenting findings through interactive and accessible visualisation methods in a transparent and reproducible way is an effective form of reaching audiences that might not otherwise realise the value of the topic or data at hand. The resulting ‘R toolbox for journalists’ is an accessible open-source resource. It can also be adapted to accommodate the need to provide a deeper understanding of the potential for data proficiency to other professions. The accessibility of R allows for users to build support communities, which in the case of journalists is essential for information gathering. Establishing and implementing transparent channels of communication is the key to scrupulous journalism and is why R is so applicable to this objective. 1.3.37 What’s New in ShinyProxy Tobias Verbeke, Managing Director, Open Analytics Track(s): R Dataviz &amp; Shiny Abstract: Shiny is nice technology to write interactive R-based applications. It is broadly adopted and the R community has collaborated on many interesting extensions. Until recently, though, deployments in larger organizations and companies required proprietary solutions. ShinyProxy fills this gap and offers a fully open source alternative to run and manage shiny applications at large. In this talk we detail the ShinyProxy architecture and demonstrate how it meets the needs of organizations. We will discuss how it scales to thousands of concurrent users and how it offers authentication and authorization functionality using standard technologies (LDAP, ActiveDirectory, OpenID Connect, SAML 2.0 and Kerberos). Also, we will discuss the management interface and how it allows to monitor application usage to collect usage statistics in event logging databases. Finally, we will demonstrate that Shiny applications can now be easily embedded in broader applications and (responsive) web sites using the ShinyProxy API. Learn how academic institutions, governmental organizations and industry roll out Shiny apps with ShinyProxy and how you can do this too. See https://shinyproxy.io. "],
["shiny-demos.html", "1.4 Shiny demos", " 1.4 Shiny demos 1.4.1 Visualising and Modelling Bike Sharing Mobility usage in the city of Milan Agostino Torti, PhD student - Politecnico di Milano Track(s): R Dataviz &amp; Shiny Abstract: A major trend in modern science is the collection of datasets which are not only “big” but also “complex”. This is particularly true in all sharing mobility systems where data are continuously collected at all time and they are characterised by a high number of features. To extract useful information contained in this huge mass of data, the development of novel statistical techniques and innovative visualization methods are requested. In this context, we focused on BikeMi, the main bike sharing system (BSS) in the city of Milan in Italy, and we implemented an R Shiny App to analyse and study people’s mobility behaviour across the city. Through the app, it is possible to dynamically select different parameters which allow to visualise the bike sharing flows among the districts of the city according to the hour of the day, the day of the week and the weather conditions. Moreover, a predictive model is implemented in the dashboard allowing to observe the future behaviour or the BSS. By doing this, we would like to both visualize the statistically significant spatio-temporal patterns of the users and to model each possible bike flow in the BikeMi network. Coauthor(s): Alessia Pini, Simone Vantini . 1.4.2 Media Shiny: Marketing Mix Models Builder Andrea Melloncelli, VanLog Track(s): R Dataviz &amp; Shiny, R Machine Learning &amp; Models, R Production Abstract: Marketing Mix Models are used to understand the effects of advertising campaigns. Building such models is challenging: first of all, it requires control of the external effects, such as seasonalities, competitor activities etc. Secondly, it requires to model the decay effect of communication (adstock effect: I do my advertising today, and in two weeks it still has some effect). The MediaShiny application allows to build Marketing Mix Models interactively: all steps of MMM, such as selecting, transforming and exploring features (time series), adstock control, model building, evaluation and forecasting can be done interactively. As a result the model explains the impact on sales of each media channel (tv, digital, press etc), controlling the external effects. An extra module allows the media budget optimization, using historical data to understand if the level of advertising has no impact or is too high (saturation). MediaShiny is a Package App developed with Golem and modularized with Shiny modules. Automatically built and provisioned as a Docker image running in a Shiny Proxy instance. Best User Experience is provided with Drag and Drop and navigation guided by action buttons. Coauthor(s): Mariachiara Fortuna . 1.4.3 ESPRES: A shiny web tool to support River Basin Management planning in European Watersheds Angel Udias, European Commission, Joint Research Centre (JRC), Ispra, Italy Track(s): R Life Sciences Abstract: Integrated river basin management must meet environmental targets while preserving the economic activities of its communities. Stakeholder decisions need to consider conflicting trade-offs between legislative environmental targets and economic activities, while maintaining a basis of transparency and accountability. ESPRES is a shiny web-based Decision Support Tool (DST) that can be used by stakeholders to explore management options in European water bodies. The management options considered in ESPRES are related with the pressures (water use and nutrient application) reduction. The shiny web interface provides a point of access to perform analyses of efficient pressure reduction strategies reflecting their perception of costs/effort, political difficulty, and social acceptability of the available solutions. Stakeholders express preferences and perceived difficulties in addressing each environmental pressure by assigning relative weights. The tool include a MOO engine to identified Pareto efficient strategies in terms of maximize the quality in the basin minimizing the total effort for reducing the pressures An online version of ESPRES is currently available (www.espres.eu) for four European basins of the Globaqua project, namely the Adige, the Ebro, the Evrotas, and the Sava, to addresses water abstraction and nutrient pollution pressures. Coauthor(s): A. Udias, B. Grizzetti, F. Bouraoui, O Vigiak, A. Pistocchi . 1.4.4 tsviz: a data-scientist-friendly addin for RStudio Emanuele Fabbiani, Chief Data Scientist at xtream, PhD student in Machine Learning Track(s): R Dataviz &amp; Shiny Abstract: In recent years, charting libraries have evolved following two main directions. First, they provided users with as many features as possible and second, they added high-level APIs to easily create the most frequent visualizations. RStudio, with its addins, offers the opportunity to further ease the creation of common plots. Born as an internal project in xtream, tsviz is an open-source Shiny-based addin which contains powerful tools to perform explorative analysis of multivariate time series. Its usage is dead simple. Once launched, it scans the global environment for suitable variables. You chose one, and several plots of the time series are shown. Line charts, scatter plots, autocorrelogram, periodogram are only a few examples. Interactivity is achieved by the miniUI framework and the adoption of Plotly charts. Its wide adoption among our customers and the overall positive feedback we received demonstrate how addins, usually thought as shortcuts for developers, may provide effective support to data scientists in performing their routine tasks. Coauthor(s): Marta Peroni, Riccardo Maganza . 1.4.5 Mobility scan Josue Aduna, Behavioural and data scientist at Livemobility Track(s): R Dataviz &amp; Shiny Abstract: This is a Shiny application designed and developed to foster sustainable mobility behavior under a specific initiative that I currently work in: Livemobility (see https://www.livemobility.com/). Broadly speaking, Livemobility is a platform that rewards people for sustainable commuting behavior and helps companies to save money, avoid environmental pollution, improve public health and save travel time. This is achieved through a digital ecosystem that analyses mobility behavior and generates personalized insights to improve mobility efficiency. The Shiny application makes use of web interactive settings together with Google Maps APIs to provide relevant indicators of impact, generate geographic scans and create mobility profiles. 1.4.6 Developing Shiny applications to facilitate precision agriculture workflows Lorenzo Busetto, Institute on Remote Sensing of Enviroment - National Research Council of Italy (CNR-IREA) Track(s): R Applications, R Dataviz &amp; Shiny Abstract: Precision Agriculture applications rely on geospatial datasets from heterogeneous sources such as crop maps, information about fertilization/phytosanitary treatments, satellite and meteo data, to optimize agricultural practices from an economic and environmental standpoint. Software instruments allowing to easily record, manage and process such datasets are therefore of paramount importance to facilitate, standardize and speed-up the steps required to implement specific workflows. Although required functionalities are available in open source/commercial software, technicians are often required to use different software tools. This affects the time and effort required to replicate a specific workflow on different areas and crop seasons. In this contribution we present our experience in developing two Shiny-based prototypes specifically tailored to the needs of operators of a agro-consulting firm providing precision agriculture services. The first prototype is mainly aimed at providing a simplified, standardized and scalable way to insert, record and query information about agricultural practices, such as crop type/variety, fertilisation and phytosanitary treatments and yield. The second is instead dedicated to facilitating access to satellite imagery data, and applying dedicated processing algorithms for identification of homegenous Management Unit Zones. Coauthor(s): Luigi Ranghetti, Donato Cillis, Maddalena Campi, Saverio Zagaglia, Gabriele Dottori, MIrco Boschetti . 1.4.7 “GUInterp”: a Shiny GUI to support spatial interpolation Luigi Ranghetti, Institute for Remote Sensing of Environment, Consiglio Nazionale delle Ricerche (IREA-CNR) Track(s): R Dataviz &amp; Shiny Abstract: In this demo we present “GUInterp”, a Shiny interface written to facilitate the operations required to interpolate point data. A typical spatial interpolation workflow includes common steps: loading point data, filtering them to exclude undesired outlier values, setting the interpolation method and parameters, defining an output raster grid and processing data. Interpolation can be conducted in R using dedicated packages; nevertheless, the availability of an interactive interface could be useful to provide additional control during steps requiring user intervention and to facilitate users with low or no programming skills. “GUInterp” was written for this purpose. The user can import input point data, optionally loading a polygon dataset of borders used to constrain the extent of the interpolated outputs. A set of selectors allows filtering input points based on the distribution of the variable to interpolate (which is shown with a reactive histogram) or the spatial position of points (visible on a map). The interpolation can be performed with IDW or Ordinary Kriging methods: in the latter case, the semivariogram can be interactively defined and optimised using a dedicated interface. Further settings can be exploited to tune computation requirements (RAM usage, amount of time) on the basis of available hardware or user needs. “GUInterp” is released as R package under the GNU GPL-3 license. Coauthor(s): Luigi Ranghetti, Mirco Boschetti, Donato Cillis, Lorenzo Busetto . 1.4.8 A demonstration of ABACUS: Apps Based Activities for Communicating and Understanding Statistics Mintu Nath, Medical Statistics Team, Institute of Applied Health Sciences, University of Aberdeen, AB25 2ZD, UK Track(s): R Dataviz &amp; Shiny Abstract: ABACUS, developed using Shiny framework, is a set of applications for effective communication and understanding in statistics. It is currently available as an R package. Users who are not familiar with R programming can also access the applications through its web-based interface. The current version of ABACUS includes properties of Normal distribution, properties of the sampling distribution, one-sample z and t tests, two samples unpaired t-test and analysis of variance and comparison of Normal and t distributions. Using an example, the shiny demonstration will include the essential features of the application particularly its relevance in generating data across wide-ranging disciplines, its interactive elements and identifying best practices for presentation of results and interpretation of statistical outputs. 1.4.9 Scoring the Implicit Association Test has never been easier: DscoreApp Ottavia M. Epifania, University of Padova (IT) Track(s): R Dataviz &amp; Shiny Abstract: Throughout the past decades, the interest in the implicit investigation of attitudes and preferences has been constantly growing among social scientists, and the Implicit Association Test (IAT) is one of the most common measures used for this aim. The so-called “IAT effect” (i.e., the difference in respondents’ performance between two contrasting categorization tasks) is usually expressed by the D-score. Despite that several options exist for computing the D-score, including R packages and SPSS syntaxes, none of them provides either an easy to use interface or a means for immediately visualizing the results. A Shiny Web application (DscoreApp) was developed to provide IAT users with an easy to use and powerful tool for the computation of the D-score. DscoreApp allows users to upload their IAT data, decide which specific D-score algorithm to compute, and immediately see the results in easy to read and interactive graphs. At the end of the computation, users can download a data frame containing the computed D-score and other information on respondents’ performance, such as the proportion of correct responses or the number of trials exceeding a time threshold. Graphical representations can be downloaded as well. Besides providing an easy to use and open source tool for computing the D-score, DscoreApp allows for grasping an immediate overview of the results, and to visually inspect them. Coauthor(s): Anselmi Pasquale, Robusto Egidio . 1.4.10 rTRhexNG: Hexagon sticker app for rTRNG Riccardo Porreca, R Enthusiast at Mirai Solutions Track(s): R Dataviz &amp; Shiny Abstract: Hexagon stickers have become a popular way to make software tools, and R packages in particular, visually recognizable and stand out as landmarks in an ever-growing ecosystem. In general, good hexagon logos are not only visually appealing but also convey the key aspects of a package with their graphical design. In this talk, we will showcase rTRhexNG (https://github.com/miraisolutions/rTRhexNG#readme), a Shiny app built for creating the hexagon sticker of the rTRNG (https://github.com/miraisolutions/rTRNG#readme) package. The core idea behind the logo was to have an appealing design that would at the same time illustrate the key features of the package: jump and split operations on (pseudo-)random sequences. Leveraging on the simple yet powerful SVG image format, R was used to automate the creation and location of several visual elements representing random sequences, and a Shiny app was built on top to quickly assess different designs in an interactive way. We demonstrate the Shiny app in action to concretely explain what jump and split mean in rTRNG, and show how the sticker design naturally emerges from their visual representation. The power of this interactive yet automated approach was invaluable to fine-tune the final look of the sticker, also allowing to easily explore alternative polygon or circle designs the implementation naturally extends to. "]
]
